# ScriptHut Configuration
# Copy this file to scripthut.yaml and customize for your environment

# Remote backends to monitor
backends:
  # Slurm cluster example
  - name: hpc-cluster
    type: slurm
    ssh:
      host: slurm-login.cluster.edu
      port: 22
      user: your_username
      key_path: ~/.ssh/id_rsa
      # cert_path: ~/.ssh/id_rsa-cert.pub  # Optional: for certificate-based auth
      # known_hosts: ~/.ssh/known_hosts    # Optional: enable host key checking
    account: pi-faculty  # Slurm account for job submission (e.g., phd, pi-faculty)
    # login_shell: true  # Use #!/bin/bash -l to source .bash_profile in jobs

  # Multiple backends can be defined
  # - name: secondary-cluster
  #   type: slurm
  #   ssh:
  #     host: slurm2.cluster.edu
  #     user: your_username
  #     key_path: ~/.ssh/cluster2_key
  #   account: phd  # Slurm account for this backend

  # ECS backend example (not yet implemented)
  # - name: production-ecs
  #   type: ecs
  #   aws:
  #     profile: my-aws-profile  # AWS CLI profile name
  #     region: us-east-1
  #     cluster_name: my-ecs-cluster

# Git repositories containing job definitions
sources:
  # Example: ML pipeline jobs
  - name: ml-jobs
    url: git@github.com:your-org/ml-pipelines.git
    branch: main
    deploy_key: ~/.ssh/ml-jobs-deploy-key

  # Example: Data processing jobs
  # - name: data-processing
  #   url: git@github.com:your-org/data-jobs.git
  #   branch: main
  #   deploy_key: ~/.ssh/data-deploy-key

# Workflows for batch job submission
# These are SSH commands that return JSON task lists
# Trigger them via the UI to create a queue of jobs
workflows:
  # Example: ML training pipeline
  - name: ml-training
    backend: hpc-cluster          # Which backend to submit jobs to
    command: "python /shared/scripts/get_training_tasks.py"
    max_concurrent: 5             # Max jobs running at once
    description: "ML model training pipeline"

  # Example: Git workflow â€” clones a repo on the backend, then runs the command
  # inside the cloned directory to generate the task list
  # - name: ml-training-git
  #   backend: hpc-cluster
  #   git:
  #     repo: git@github.com:your-org/ml-pipelines.git
  #     branch: main
  #     deploy_key: ~/.ssh/ml-deploy-key  # local path, uploaded to backend temporarily
  #     clone_dir: ~/scripthut-repos      # parent dir on the backend (default: ~/scripthut-repos)
  #   command: "python get_tasks.py"
  #   max_concurrent: 5
  #   description: "ML training from git repo (clones on backend)"

  # Example: Data processing batch
  # - name: data-processing
  #   backend: hpc-cluster
  #   command: "cat /shared/tasks/data_pipeline.json"
  #   max_concurrent: 10
  #   description: "Daily data processing batch"

# Workflow commands must return JSON in this format:
# {
#   "tasks": [
#     {
#       "id": "task-001",
#       "name": "train-model-v1",
#       "command": "python train.py --config config.yaml",
#       "working_dir": "/home/user/project",
#       "partition": "gpu",
#       "cpus": 4,
#       "memory": "16G",
#       "time_limit": "4:00:00",
#       "output_file": "/path/to/custom/output.log",  # Optional: custom stdout path
#       "error_file": "/path/to/custom/error.log",    # Optional: custom stderr path
#       "env_vars": {"MY_PARAM": "42", "DATA_DIR": "/scratch/data"}  # Optional: per-task env vars
#     }
#   ]
# }
#
# If output_file/error_file are not specified, logs will be written to:
#   /tmp/scripthut/{queue_id}/{task_id}.out
#   /tmp/scripthut/{queue_id}/{task_id}.err

# Global settings
settings:
  # How often to poll for job updates (seconds, minimum 5)
  poll_interval: 60

  # Web server binding
  server_host: 127.0.0.1
  server_port: 8000

  # Directory to cache cloned repositories
  sources_cache_dir: ~/.cache/scripthut/sources

  # Filter jobs by username (shows toggle button in UI)
  # Set to your username to enable "My Jobs" filter
  filter_user: your_username
